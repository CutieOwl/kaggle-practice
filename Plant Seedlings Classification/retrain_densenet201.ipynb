{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "This code is designed to run with dlws/tutorial-tensorflow:1.5\n",
    "\n",
    "```\n",
    "'''\n",
    "\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.callbacks import *\n",
    "import os.path\n",
    "\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import sys, inspect\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "sys.path.append(os.path.join( home, \"code/DLWorkspace-Utils/keras-multiprocess-image-data-generator\") )\n",
    "import tools.image as T\n",
    "import multiprocessing\n",
    "n_process = 16\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=n_process)\n",
    "\n",
    "# clsmembers = inspect.getmembers(sys.modules[\"keras.applications.inception_v3\"], inspect.isclass)\n",
    "# print( \"Classes === %s\" % clsmembers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3368e80fc787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# prepare data augmentation configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m train_datagen = T.ImageDataGenerator(\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mshear_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "import pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "class Tee(object):\n",
    "    def __init__(self, name):\n",
    "        self.file = open(name, \"w\")\n",
    "        self.stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "    def __del__(self):\n",
    "        sys.stdout = self.stdout\n",
    "        self.file.close()\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "#Inception input size\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "top_layers_checkpoint_path = 'cp.top.best.hdf5'\n",
    "fine_tuned_checkpoint_path = 'cp.fine_tuned.best.hdf5'\n",
    "new_extended_inception_weights = './logs/final_weights.hdf5'\n",
    "\n",
    "root_dir = '/data/isic/isic/isic'\n",
    "data_dir = os.path.join( root_dir, 'isic/2018/task3')\n",
    "\n",
    "attach_layer = \"dense_1\"\n",
    "\n",
    "lamd_final = 0.00\n",
    "lamd = 0.0\n",
    "activity_lamd = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shift = 0.1\n",
    "zoom = 0.1\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = T.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    rotation_range=90,\n",
    "    width_shift_range = shift,\n",
    "    height_shift_range = shift,\n",
    "    zoom_range=zoom,\n",
    "    horizontal_flip=True, \n",
    "    vertical_flip=True, \n",
    "    pool = pool\n",
    "    )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255, pool = pool )\n",
    "\n",
    "classes = [\"1.MEL\",\"2.NV\",\"3.BCC\",\"4.AKIEC\",\"5.BKL\",\"6.DF\",\"7.VASC\"]\n",
    "\n",
    "nb_train_samples = 8096 \n",
    "nb_validation_samples = 1536 \n",
    "\n",
    "from keras.regularizers import * \n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "from keras.optimizers import *\n",
    "\n",
    "# opt = SGD(lr = lr_top, decay=0, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "train_schedule = [(20, 0.01), (20, 0.001), (20, 0.0001) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Save the TensorBoard logs.\n",
    "import time\n",
    "now = time.strftime(\"%c\")\n",
    "\n",
    "import gc\n",
    "\n",
    "optimizer_collections = {\n",
    "    \"adadelta\" : Adadelta(), \n",
    "    \"nadam\" : Nadam(), \n",
    "    \"rmsprop\": RMSprop(), \n",
    "    \"adam\": Adam(), \n",
    "    \"adagrad\": Adagrad(), \n",
    "    \"adamax\": Adamax(), \n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', default=32, help='Batch size', type=int)\n",
    "    parser.add_argument('--top_epochs', default=3, help='Number of Epochs(Top training)', type=int)\n",
    "    parser.add_argument('--nb_epochs', default=60, help='Number of Epochs', type=int)\n",
    "    parser.add_argument('--optimizer', default='adadelta', help='Optimizer', type=str)\n",
    "    parser.add_argument('--split', default=None, help='Which data set to use', type=int)\n",
    "    parser.add_argument('--decay', default=0.0, help='Rate decay', type=float)\n",
    "    parser.add_argument('--gpus', default=1, help='Number of GPU', type=int)\n",
    "    parser.add_argument('--drops_epochs', default=0, help='Epochs which rate drop by x10', type=float)\n",
    "    parser.add_argument('--full_connects', default=1, help='Number of fully connected layer', type = int)\n",
    "\n",
    "    # parser.add_argument('model', help='Model to evaluate', type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print( args )\n",
    "    batch_size = 32\n",
    "    if args.batch_size:\n",
    "        batch_size = args.batch_size\n",
    "    \n",
    "    num_gpu = args.gpus\n",
    "    # Number of training epochs\n",
    "    fit_epochs = args.nb_epochs\n",
    "    # Number of top epochs\n",
    "    top_epochs = args.top_epochs\n",
    "    # data to use. \n",
    "    split = args.split\n",
    "    full_connects = args.full_connects\n",
    "    \n",
    "    lr_top = 0.01\n",
    "    \n",
    "    if args.optimizer.startswith(\"sgd\"):\n",
    "        optimizer = args.optimizer\n",
    "        opt = SGD(lr = lr_top, decay=args.decay, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        optimizer = args.optimizer\n",
    "        opt = optimizer_collections[args.optimizer]\n",
    "        \n",
    "    \n",
    "    if split is None:\n",
    "        parser.print_help()\n",
    "        exit(-1)\n",
    "        \n",
    "    callbacks = []\n",
    "    \n",
    "    if args.drops_epochs > 0:\n",
    "        epochs_drop = args.drops_epochs\n",
    "        def step_decay(epoch, lr):\n",
    "            initial_lrate = lr_top\n",
    "            drop = 0.1\n",
    "            lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "            # lrate = initial_lrate * math.pow(drop, (1+epoch)/epochs_drop)\n",
    "            print (\"Learning rate at epoch %d === %f \" % (epoch, lrate))\n",
    "            return lrate\n",
    "        # learning schedule callback\n",
    "        lrate = LearningRateScheduler(step_decay)\n",
    "        callbacks.append( lrate )\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_data_dir = os.path.join( root_dir, 'isic/2018/task3_split6/split%d/train' % split )\n",
    "    validation_data_dir = os.path.join( root_dir,'isic/2018/task3_split6/split%d/val' %split )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        classes = classes,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        classes = classes,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    print(train_generator)\n",
    "    print(type(train_generator))\n",
    "    \n",
    "    # Instantiate the base model (or \"template\" model).\n",
    "    # We recommend doing this with under a CPU device scope,\n",
    "    # so that the model's weights are hosted on CPU memory.\n",
    "    # Otherwise they may end up hosted on a GPU, which would\n",
    "    # complicate weight sharing.\n",
    "    if num_gpu <= 1:\n",
    "        model_store = \"/gpu:0\" # use /gpu:0 for single GPU training\n",
    "    else:\n",
    "        model_store = \"/cpu:0\"\n",
    "    \n",
    "    with tf.device(model_store):\n",
    "        # create the base pre-trained model\n",
    "        base_model = DenseNet201(weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "    model_dir = os.path.join( root_dir, \"result/densenet201_%s_batch%s_gpu1_s%d\" % ( optimizer, batch_size, split) )\n",
    "    log_dir = os.path.join( model_dir, \"logs\")\n",
    "    os.system( \"mkdir -p %s\" % log_dir)\n",
    "    log_file = os.path.join( log_dir, \"training_log\")\n",
    "    \n",
    "    sys.stdout = Tee( log_file )\n",
    "\n",
    "    import re\n",
    "    exp = re.compile( \"res.*branch.*\" )\n",
    "    with tf.device(model_store):\n",
    "        for i, layer in enumerate(base_model.layers):\n",
    "            layer.trainable = False\n",
    "            if exp.match( layer.name ):\n",
    "                # print (\"Regularize %s\" % layer.name)\n",
    "                layer.kernel_regularizer = l2(lamd)\n",
    "\n",
    "        # and a logistic layer -- we have 7 classes\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        if full_connects > 0:\n",
    "            for i in range(full_connects):\n",
    "                # let's add a fully-connected layer\n",
    "                x = Dense(1024, activation='relu', kernel_regularizer = l2(lamd_final), activity_regularizer = l1(activity_lamd), trainable = True )(x)\n",
    "        # and a logistic layer -- let's say we have 200 classes\n",
    "        predictions = Dense(7, activation='softmax', kernel_regularizer = l2(lamd_final), activity_regularizer = l1(activity_lamd), trainable = True)(x)\n",
    "\n",
    "        # this is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        # Training new top\n",
    "        # Replicates the model to multiple GPUs.\n",
    "        # This assumes that your machine has 8 available GPUs.\n",
    "    if True:\n",
    "        if num_gpu <= 1:\n",
    "            parallel_model = model #multi_gpu_model(model, gpus=num_gpu)\n",
    "        else:\n",
    "            parallel_model = multi_gpu_model(model, gpus=num_gpu )\n",
    "        \n",
    "        \n",
    "        parallel_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'] )\n",
    "        \n",
    "        # train the model on the new data for a few epochs\n",
    "        #model.fit_generator(...)\n",
    "        #class_weights = {\"1.MEL\":0.082103321,\"2.NV\":0.013375413,\"3.BCC\":0.203661327,\"4.AKIEC\":0.304794521,\"5.BKL\":0.086914063,\"6.DF\":1,\"7.VASC\":0.760683761}\n",
    "        class_weights = {0:0.082103321,1:0.013375413,2:0.203661327,3:0.304794521,4:0.086914063,5:1,6:0.760683761}\n",
    "\n",
    "        if top_epochs > 0:\n",
    "            history = parallel_model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples // batch_size,\n",
    "                epochs=top_epochs,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples // batch_size,\n",
    "                class_weight = class_weights,\n",
    "                callbacks=callbacks)\n",
    "            del history \n",
    "        \n",
    "        for layer in model.layers:\n",
    "           layer.trainable = True\n",
    "        \n",
    "        # we need to recompile the model for these modifications to take effect\n",
    "        # we use SGD with a low learning rate\n",
    "        # opt_fit = SGD(lr = lr_top, decay=0, momentum=0.9, nesterov=True)\n",
    "        if num_gpu <= 1:\n",
    "            parallel_model = model #multi_gpu_model(model, gpus=num_gpu)\n",
    "        else:\n",
    "            print(\"Initialize multiple gpu === %d\" % num_gpu )\n",
    "            parallel_model = multi_gpu_model(model, gpus=num_gpu, cpu_merge=False )\n",
    "\n",
    "        opt_fit = opt\n",
    "        parallel_model.compile(optimizer=opt_fit, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "        # alongside the top Dense layers\n",
    "        #model.fit_generator(...)\n",
    "\n",
    "        history = parallel_model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            nb_epoch=fit_epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size,\n",
    "            class_weight = class_weights,\n",
    "            callbacks=callbacks )\n",
    "        \n",
    "        model.save(os.path.join(log_dir,'isic_final.h5'))\n",
    "        \n",
    "        res = model.predict_generator(validation_generator, steps = nb_validation_samples // batch_size )\n",
    "        pred = np.argmax(res,axis=1)\n",
    "        result = T.ClassificationResults(pred, validation_generator.classes, 7)\n",
    "        print( \"Model ==== %s \" % model_dir )\n",
    "        print( \"Precision === %s\" % np.mean(result.precision) )\n",
    "        print( \"Recall === %s\" % np.mean(result.recall) )\n",
    "        print( \"F1 === %s\" % np.mean(result.f1) )\n",
    "        \n",
    "        # del history\n",
    "        # del parallel_model \n",
    "        # del model\n",
    "        # del base_model\n",
    "        # gc.collect()\n",
    "        # K.clear_session() \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
