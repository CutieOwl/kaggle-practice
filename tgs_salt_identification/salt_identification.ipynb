{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model of Salt Identification with Keras ConvNet\n",
    "Transfer learning will probably be harder with the wonky pictures of salt identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# % matplotlib inline\n",
    "import datetime as dt\n",
    "\n",
    "# Import plotting for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 16\n",
    "#from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pydot\n",
    "import random, math\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from keras.applications import xception\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Concatenate, UpSampling2D\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "import sys\n",
    "sys.path.append(\"/utils/DLWorkspace-Utils/keras-multiprocess-image-data-generator/tools\")\n",
    "import image as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix # for the confusion matrix\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import warnings\n",
    "import cv2\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's set up our optimizer so we can choose which one later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, decay=0.0006, dropout=0.5, drops_epochs=0, epsilon=1e-06, gpus=1, lr=0.003, nb_epochs=30, optimizer='adam', pooltype='max', reducelr=0, rho=0.95, split=None, upsample=0)\n"
     ]
    }
   ],
   "source": [
    "optimizer_collections = {\n",
    "    \"adadelta\" : Adadelta(), \n",
    "    \"nadam\" : Nadam(), \n",
    "    \"rmsprop\": RMSprop(), \n",
    "    \"adam\": Adam(), \n",
    "    \"adagrad\": Adagrad(), \n",
    "    \"adamax\": Adamax(), \n",
    "}\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=16, help='Batch size', type=int)\n",
    "parser.add_argument('--nb_epochs', default=30, help='Number of Epochs', type=int)\n",
    "parser.add_argument('--optimizer', default='adam', help='Optimizer', type=str)\n",
    "parser.add_argument('--split', default=None, help='Which data set to use', type=int)\n",
    "parser.add_argument('--decay', default=6e-4, help='Rate decay', type=float)\n",
    "parser.add_argument('--gpus', default=1, help='Number of GPU', type=int)\n",
    "parser.add_argument('--drops_epochs', default=0, help='Epochs which rate drop by x10', type=float)\n",
    "parser.add_argument('--lr', default=3e-3, help='Learning Rate', type=float)\n",
    "parser.add_argument('--epsilon', default=1e-6, help='Optimizer Epsilon', type=float)\n",
    "parser.add_argument('--rho', default=0.95, help='Optimizer Rho', type=float)\n",
    "parser.add_argument('--dropout', default=0, help='Dropout', type=float)\n",
    "parser.add_argument('--reducelr', default=0, help='Reduce learning rate? 0 or 1', type=int)\n",
    "parser.add_argument('--pooltype', default='max', help='avg or max pool', type=str)\n",
    "parser.add_argument('--upsample', default=0, help='1? upsample:conv2dtranspose', type=int)\n",
    "\n",
    "args = parser.parse_args(\"--optimizer adam --dropout 0.5\".split())\n",
    "#args = parser.parse_args()\n",
    "\n",
    "print( args )\n",
    "BATCH_SIZE = 8\n",
    "if args.batch_size:\n",
    "    BATCH_SIZE = args.batch_size\n",
    "\n",
    "num_gpu = args.gpus\n",
    "# Number of training epochs\n",
    "EPOCHS = args.nb_epochs\n",
    "# data to use. \n",
    "split = args.split\n",
    "\n",
    "DO = args.dropout\n",
    "\n",
    "LR = args.lr\n",
    "DECAY = args.decay\n",
    "EPS = args.epsilon\n",
    "RHO = args.rho\n",
    "\n",
    "if args.optimizer.startswith(\"sgd\"):\n",
    "    optimizer = args.optimizer\n",
    "    opt = SGD(lr = LR, decay=DECAY, momentum=0.9, nesterov=True)\n",
    "elif args.optimizer.startswith(\"adam\"):\n",
    "    optimizer = args.optimizer\n",
    "    opt = Adam(lr=LR, beta_1=0.9, beta_2=0.999, epsilon=EPS, decay=DECAY)\n",
    "elif args.optimizer.startswith(\"adadelta\"):\n",
    "    optimizer = args.optimizer\n",
    "    opt = Adadelta(lr=LR, rho=RHO, epsilon=EPS, decay=DECAY)  \n",
    "elif args.optimizer.startswith(\"rmsprop\"):\n",
    "    optimizer = args.optimizer\n",
    "    opt = RMSprop(lr=LR, rho=RHO, epsilon=EPS, decay=DECAY)\n",
    "else:\n",
    "    optimizer = args.optimizer\n",
    "    opt = optimizer_collections[args.optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a tee object to help write log output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tee(object):\n",
    "    def __init__(self, name):\n",
    "        self.file = open(name, \"w\")\n",
    "        self.stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "    def __del__(self):\n",
    "        sys.stdout = self.stdout\n",
    "        self.file.close()\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "    def flush(self):\n",
    "        self.file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the dataset\n",
    "First, we locate our data at the correct directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/kaggle/competitions/tgs-salt-identification-challenge/train\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/kaggle/competitions/tgs-salt-identification-challenge/'\n",
    "cur_dir = '/work/kaggle-practice/tgs_salt_identification/'\n",
    "result_dir = os.path.join(cur_dir, 'result')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "train_dir_img = os.path.join(train_dir, 'images')\n",
    "train_dir_mask = os.path.join(train_dir, 'masks')\n",
    "print(train_dir)\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "test_dir = os.path.join(test_dir, 'images')\n",
    "#sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n",
    "IMAGE_SIZE = 128 # pixel height and width of each image--actual size of the images is 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a little piece of code if we wanted to test out different lrs and decays\n",
    "```python\n",
    "if os.path.isdir(os.path.join(result_dir, \"tgs-salt_%s_batch%s_epoch%s_lr%s_decay%s\" % (optimizer, batch_size, fit_epochs, lr_top, args.decay))) == False:\n",
    "    os.mkdir(os.path.join(result_dir, \"tgs-salt_%s_batch%s_epoch%s_lr%s_decay%s\" % (optimizer, batch_size, fit_epochs, lr_top, args.decay)))\n",
    "    \n",
    "model_dir = os.path.join(result_dir, \"tgs-salt_%s_batch%s_epoch%s_lr%s_decay%s\" % (optimizer, batch_size, fit_epochs, lr_top, args.decay))\n",
    "```\n",
    "\n",
    "And this was the naming before I split it up\n",
    "```python\n",
    "\"8.14-tgs-salt_%s_batch%s_epoch%s\" % (optimizer, BATCH_SIZE, EPOCHS)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if os.path.isdir(os.path.join(result_dir, prefix + \"-tgs-salt\"  + info)) == False:\\n    os.mkdir(os.path.join(result_dir, prefix + \"-tgs-salt\"  + info))\\nmodel_dir = os.path.join(result_dir, prefix + \"-tgs-salt\"  + info)\\nif os.path.exists(os.path.join(model_dir, \"training_log\")) == True:\\n    os.remove(os.path.join(model_dir, \"training_log\"))\\nlog_file = os.path.join(model_dir, \"training_log\")\\nprint(prefix + \"-tgs-salt\"  + info)\\nsys.stdout = Tee(log_file)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up location of output\n",
    "prefix = \"8.27\"\n",
    "info = (\"_%s_batch%s_epoch%s_dropout%s_pool%s_upsample%s\" % (optimizer, BATCH_SIZE, EPOCHS, DO, args.pooltype, args.upsample))\n",
    "'''if os.path.isdir(os.path.join(result_dir, prefix + \"-tgs-salt\"  + info)) == False:\n",
    "    os.mkdir(os.path.join(result_dir, prefix + \"-tgs-salt\"  + info))\n",
    "model_dir = os.path.join(result_dir, prefix + \"-tgs-salt\"  + info)\n",
    "if os.path.exists(os.path.join(model_dir, \"training_log\")) == True:\n",
    "    os.remove(os.path.join(model_dir, \"training_log\"))\n",
    "log_file = os.path.join(model_dir, \"training_log\")\n",
    "print(prefix + \"-tgs-salt\"  + info)\n",
    "sys.stdout = Tee(log_file)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"), index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(os.path.join(data_dir, \"depths.csv\"), index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 959, Min: 51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eeaccaa0354336afd3ced964e5d3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fadd823716943a1b1d084ecd0ec596a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2511ba0ce34492be81766bde604c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(101, 101)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5910c80f1664a3c9c15df79d09d2f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(101, 101, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z</th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "      <th>depth</th>\n",
       "      <th>images_d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>575d24d81d</th>\n",
       "      <td>843</td>\n",
       "      <td>[[0.5254901960784314, 0.5137254901960784, 0.52...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.8722466960352423, 0.8722466960352423, 0.87...</td>\n",
       "      <td>[[[0.5254901960784314, 0.8722466960352423], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a266a2a9df</th>\n",
       "      <td>794</td>\n",
       "      <td>[[0.3411764705882353, 0.3764705882352941, 0.33...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.8182819383259912, 0.8182819383259912, 0.81...</td>\n",
       "      <td>[[[0.3411764705882353, 0.8182819383259912], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75efad62c1</th>\n",
       "      <td>468</td>\n",
       "      <td>[[0.5686274509803921, 0.4666666666666667, 0.32...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.4592511013215859, 0.4592511013215859, 0.45...</td>\n",
       "      <td>[[[0.5686274509803921, 0.4592511013215859], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34e51dba6a</th>\n",
       "      <td>727</td>\n",
       "      <td>[[0.5411764705882353, 0.4745098039215686, 0.39...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.7444933920704846, 0.7444933920704846, 0.74...</td>\n",
       "      <td>[[[0.5411764705882353, 0.7444933920704846], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875705fb0</th>\n",
       "      <td>797</td>\n",
       "      <td>[[0.06666666666666667, 0.0784313725490196, 0.0...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.8215859030837004, 0.8215859030837004, 0.82...</td>\n",
       "      <td>[[[0.06666666666666667, 0.8215859030837004], [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              z                                             images  \\\n",
       "id                                                                   \n",
       "575d24d81d  843  [[0.5254901960784314, 0.5137254901960784, 0.52...   \n",
       "a266a2a9df  794  [[0.3411764705882353, 0.3764705882352941, 0.33...   \n",
       "75efad62c1  468  [[0.5686274509803921, 0.4666666666666667, 0.32...   \n",
       "34e51dba6a  727  [[0.5411764705882353, 0.4745098039215686, 0.39...   \n",
       "4875705fb0  797  [[0.06666666666666667, 0.0784313725490196, 0.0...   \n",
       "\n",
       "                                                        masks  \\\n",
       "id                                                              \n",
       "575d24d81d  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "a266a2a9df  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "75efad62c1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "34e51dba6a  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4875705fb0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                        depth  \\\n",
       "id                                                              \n",
       "575d24d81d  [[0.8722466960352423, 0.8722466960352423, 0.87...   \n",
       "a266a2a9df  [[0.8182819383259912, 0.8182819383259912, 0.81...   \n",
       "75efad62c1  [[0.4592511013215859, 0.4592511013215859, 0.45...   \n",
       "34e51dba6a  [[0.7444933920704846, 0.7444933920704846, 0.74...   \n",
       "4875705fb0  [[0.8215859030837004, 0.8215859030837004, 0.82...   \n",
       "\n",
       "                                                     images_d  \n",
       "id                                                             \n",
       "575d24d81d  [[[0.5254901960784314, 0.8722466960352423], [0...  \n",
       "a266a2a9df  [[[0.3411764705882353, 0.8182819383259912], [0...  \n",
       "75efad62c1  [[[0.5686274509803921, 0.4592511013215859], [0...  \n",
       "34e51dba6a  [[[0.5411764705882353, 0.7444933920704846], [0...  \n",
       "4875705fb0  [[[0.06666666666666667, 0.8215859030837004], [...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_DEPTH = max(train_df[\"z\"])\n",
    "MIN_DEPTH = min(train_df[\"z\"])\n",
    "print(\"Max: %s, Min: %s\" % (MAX_DEPTH, MIN_DEPTH))\n",
    "train_df[\"images\"] = [np.array(load_img(os.path.join(train_dir_img, '%s.png' % idx), grayscale=True)) / 255 \n",
    "                      for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(os.path.join(train_dir_mask, '%s.png' % idx), grayscale=True)) / 255 \n",
    "                     for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"depth\"] = [((np.ones_like(train_df.loc[i][\"images\"]) * train_df.loc[i][\"z\"]) - MIN_DEPTH)/(MAX_DEPTH - MIN_DEPTH)\n",
    "                     for i in tqdm_notebook(train_df.index)]\n",
    "print(train_df[\"depth\"][0].shape)\n",
    "train_df[\"images_d\"] = [np.dstack((train_df[\"images\"][i], train_df[\"depth\"][i])) for i in tqdm_notebook(train_df.index)]\n",
    "print(train_df[\"images_d\"][0].shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the distribution of the data.\n",
    "Depth first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_ = sns.distplot(train_df.z, label=\"Train\")\\n_ = sns.distplot(test_df.z, label=\"Test\")\\n_ = plt.legend()\\n_ = plt.title(\"Depth distribution\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_ = sns.distplot(train_df.z, label=\"Train\")\n",
    "_ = sns.distplot(test_df.z, label=\"Test\")\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Depth distribution\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask coverage...\n",
    "Notice that most of the pictures don't have a lot of mask coverage--the data isn't even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_ = sns.distplot(train_df.masks.map(coverage_class), label=\"Train\", kde=False)\\n_ = plt.legend()\\n_ = plt.title(\"Coverage distribution\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coverage(mask):\n",
    "    \"\"\"Compute salt mask coverage\"\"\"\n",
    "    return np.sum(mask) / (mask.shape[0]*mask.shape[1])\n",
    "\n",
    "def coverage_class(mask):\n",
    "    \"\"\"Compute salt mask coverage class\"\"\"\n",
    "    return (coverage(mask) * 100 //10).astype(np.int8)\n",
    "\n",
    "'''\n",
    "_ = sns.distplot(train_df.masks.map(coverage_class), label=\"Train\", kde=False)\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Coverage distribution\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's visualize the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplot_imgs_masks(train_df.iloc[:40].images_d, train_df.iloc[:40].masks)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_imgs_masks(imgs, masks, preds_valid=None, thres=None, grid_width=10, zoom=1.5):\n",
    "    \"\"\"Visualize seismic images with their salt area mask(green) and optionally salt area prediction(pink). \n",
    "    The prediction mask can be either in probability-mask or binary-mask form(based on threshold)\n",
    "    \"\"\"\n",
    "    grid_height = 1 + (len(imgs)-1) // grid_width\n",
    "    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*zoom, grid_height*zoom))\n",
    "    axes = axs.ravel()\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        mask = masks[i]\n",
    "        depth = img[0, 0, 1]\n",
    "        \n",
    "        \n",
    "        ax = axes[i] #//grid_width, i%grid_width]\n",
    "        _ = ax.imshow(img[..., 0], cmap=\"Greys\")\n",
    "        _ = ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "        \n",
    "        if preds_valid is not None:\n",
    "            if thres is not None:\n",
    "                pred = np.array(np.round(preds_valid[i] > thres), dtype=np.float32)\n",
    "            else:\n",
    "                pred = preds_valid[i]\n",
    "            _ = ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "        \n",
    "        _ = ax.text(2, img.shape[0]-2, depth * MAX_DEPTH//1, color=\"k\")\n",
    "        _ = ax.text(img.shape[0]-2, 2, round(coverage(mask), 2), color=\"k\", ha=\"right\", va=\"top\")\n",
    "        _ = ax.text(2, 2, coverage_class(mask), color=\"k\", ha=\"left\", va=\"top\")\n",
    "        \n",
    "        _ = ax.set_yticklabels([])\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = plt.axis('off')\n",
    "    plt.suptitle(\"Green: Salt area mask \\nTop-left: coverage class, top-right: salt coverage, bottom-left: depth\", y=1+.5/grid_height, fontsize=20)\n",
    "    plt.tight_layout();\n",
    "    \n",
    "# show 40 images with their masks overlaid in green\n",
    "'''\n",
    "plot_imgs_masks(train_df.iloc[:40].images_d, train_df.iloc[:40].masks)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(img, img_size_target=IMAGE_SIZE):\n",
    "    \"\"\"Resize image to target\"\"\"\n",
    "    img_size = img.shape[0]\n",
    "    if img_size == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "\n",
    "def downsample(img, img_size_orig=101):\n",
    "    \"\"\"Resize image to original\"\"\"\n",
    "    img_size = img.shape[0]\n",
    "    if img_size == img_size_orig:\n",
    "        return img\n",
    "    return resize(img, (img_size_orig, img_size_orig), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 128, 128, 2)\n",
      "(3200, 128, 128, 1)\n",
      "(800, 128, 128, 2)\n",
      "(800, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, depth_train, depth_valid = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images_d.map(upsample).tolist()).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 2), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1), \n",
    "    train_df.z.values,\n",
    "    test_size=0.2, \n",
    "    stratify=train_df.masks.map(coverage_class), \n",
    "    random_state=1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image augmentation time!\n",
    "According to Frank on Kaggle, shifts, zooming out, and vertical flip are not useful, so I only have zooming in and horizontal flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/preprocessing/image.py:957: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3200, 128, 128, 2) (2 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n",
      "/usr/local/lib/python3.5/dist-packages/keras/preprocessing/image.py:1144: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (3200, 128, 128, 2) (2 channels).\n",
      "  ' (' + str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    }
   ],
   "source": [
    "NB_BATCHES = 300\n",
    "\n",
    "# The images and masks need to modified the same way, so their ImageDataGenerators are the same.\n",
    "image_datagen = ImageDataGenerator(zoom_range=[.9, 1],\n",
    "                                   horizontal_flip=True,)\n",
    "mask_datagen = ImageDataGenerator(zoom_range=[.9, 1],\n",
    "                                  horizontal_flip=True,)\n",
    "\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "image_datagen.fit(x_train, seed=1)\n",
    "mask_datagen.fit(y_train, seed=1)\n",
    "\n",
    "image_generator = image_datagen.flow(\n",
    "    x_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=1)\n",
    "\n",
    "mask_generator = mask_datagen.flow(\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=1)\n",
    "\n",
    "# combine generators into one which yields image and masks\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e68e27f9354df3a5bad053b78b668d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_l = []\n",
    "Y_train_l = []\n",
    "    \n",
    "# add examples to list by batch\n",
    "for batch_id, (x_batch, y_batch) in tqdm_notebook(enumerate(train_generator)):\n",
    "    # Add full batches only - prevent odd array shapes\n",
    "    if x_batch.shape[0] == BATCH_SIZE:\n",
    "        X_train_l.append(x_batch)\n",
    "        Y_train_l.append(y_batch)\n",
    "    # Break infinite loop manually when required number of batches is reached\n",
    "    if len(X_train_l) == NB_BATCHES: break\n",
    "\n",
    "# Sanity check all arrays are same shape\n",
    "assert len(set(arr.shape for arr in X_train_l)) == 1\n",
    "assert len(set(arr.shape for arr in Y_train_l)) == 1\n",
    "\n",
    "# Stack list of arrays\n",
    "X_train_augm = np.vstack(X_train_l)\n",
    "Y_train_augm = np.vstack(Y_train_l)\n",
    "\n",
    "# Sanity check stacking over first dimension\n",
    "assert X_train_augm.shape[0] == BATCH_SIZE * NB_BATCHES\n",
    "assert Y_train_augm.shape[0] == BATCH_SIZE * NB_BATCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplot_imgs_masks(np.squeeze(X_train_augm[:40]), np.squeeze(Y_train_augm[:40]))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plot_imgs_masks(np.squeeze(X_train_augm[:40]), np.squeeze(Y_train_augm[:40]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for the actual model!\n",
    "It's a mixture of a U-Net and a DenseNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, an Intersection over Union metric to calculate the accuracy of our identification.\n",
    "Remember, IoU helps in object detection by figuring out the similarity (intersection/union) of two bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My findings during testing:\n",
    "- Optimizer? **Adam**. Adadelta does best on train and validation but Adam does best on test.\n",
    "- Dropout?\n",
    "- Image size? **128 pixels** seems to be doing well. (Further testing might help)\n",
    "- Number of convblocks? **11 convblocks** is pretty good. (Further testing might help)\n",
    "- Reduce learning rate? \n",
    "- Batch size? **32 batch size** works the best.\n",
    "- Epochs? The accuracy continues to improve up to **120 epochs**; I could probably test for longer than 120 epochs and still have it improve. (Further testing might help)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My original U-Net model before melding with DenseNet:\n",
    "```python\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (s)\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other people's code:\n",
    "```python\n",
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\tn = Dropout(do)(n) if do else n\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\treturn Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "\tif depth > 0:\n",
    "\t\tn = conv_block(m, dim, acti, bn, res)\n",
    "\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "\t\tif up:\n",
    "\t\t\tm = UpSampling2D()(m)\n",
    "\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "\t\telse:\n",
    "\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "\t\tn = Concatenate()([n, m])\n",
    "\t\tm = conv_block(n, dim, acti, bn, res)\n",
    "\telse:\n",
    "\t\tm = conv_block(m, dim, acti, bn, res, do)\n",
    "\treturn m\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "\ti = Input(shape=img_shape)\n",
    "\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "\to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
    "\treturn Model(inputs=i, outputs=o)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original blocks:\n",
    "```python\n",
    "def dense_block(x, bn_axis, channel):\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = Conv2D(channel, (3, 3), padding='same', use_bias=False)(x1)\n",
    "    x1 = Conv2D(channel, (3, 3), padding='same', use_bias=False)(x1)\n",
    "    x = Concatenate(axis=bn_axis)([x, x1])\n",
    "    x = Conv2D(channel, (3, 3), padding='same', use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "def up_trans_block(x, bn_axis, channel, dropout=None):\n",
    "    t = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    t = Activation('relu')(t)\n",
    "    c = Conv2D(channel, (3, 3), activation='relu', padding='same') (t)\n",
    "    p = AveragePooling2D(2, strides=2)(c)\n",
    "    if dropout:\n",
    "        d = Dropout(dropout)(p)\n",
    "        return d\n",
    "    return p\n",
    "\n",
    "def down_trans_block(x, y, bn_axis, channel, dropout=None):\n",
    "    t = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    t = Activation('relu')(t)\n",
    "    c = Conv2DTranspose(channel, (2, 2), strides=(2, 2), padding='same') (t)\n",
    "    u = Concatenate(axis=bn_axis)([c, y])\n",
    "    if dropout:\n",
    "        d = Dropout(dropout)(u)\n",
    "        return d\n",
    "    return u\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocks before ResNet combination:\n",
    "```python\n",
    "def dense_block(x, bn_axis, channel, dropout=0):\n",
    "    x1 = Conv2D(channel, (3, 3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x1)\n",
    "    x1 = Dropout(dropout)(x1)   if dropout else x1\n",
    "    x1 = Conv2D(channel, (3, 3), activation='relu', padding='same', use_bias=False)(x1)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x1)\n",
    "    x = Concatenate(axis=bn_axis)([x, x1])\n",
    "    x = Conv2D(channel, (3, 3), padding='same', use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "def up_trans_block(x, bn_axis, channel, pool=args.pooltype):\n",
    "    t = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    c = Conv2D(channel, (3, 3), activation='relu', padding='same') (t)\n",
    "    if pool == 'avg':\n",
    "        p = AveragePooling2D(2, strides=2)(c)\n",
    "    else:\n",
    "        p = MaxPooling2D((2, 2))(c)\n",
    "    return p\n",
    "        \n",
    "def down_trans_block(x, y, bn_axis, channel, upsampling=args.upsample):\n",
    "    t = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    if upsampling:\n",
    "        c = UpSampling2D()(t)\n",
    "    else:\n",
    "        c = Conv2DTranspose(channel, (2, 2), strides=(2, 2), padding='same') (t)\n",
    "    c = Conv2D(channel, 2, activation='relu', padding='same')(c)\n",
    "    u = Concatenate(axis=bn_axis)([c, y])\n",
    "    return u\n",
    "        \n",
    "inputs = Input((IMAGE_SIZE, IMAGE_SIZE, 2))\n",
    "\n",
    "bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "start_channel = 16\n",
    "\n",
    "b1d = dense_block(inputs, bn_axis, start_channel)\n",
    "b1t = up_trans_block(b1d, bn_axis, start_channel)\n",
    "\n",
    "b2d = dense_block(b1t, bn_axis, start_channel * 2)\n",
    "b2t = up_trans_block(b2d, bn_axis, start_channel * 2)\n",
    "\n",
    "b3d = dense_block(b2t, bn_axis, start_channel * 4)\n",
    "b3t = up_trans_block(b3d, bn_axis, start_channel * 4)\n",
    "\n",
    "b4d = dense_block(b3t, bn_axis, start_channel * 8)\n",
    "b4t = up_trans_block(b4d, bn_axis, start_channel * 8)\n",
    "\n",
    "b5d = dense_block(b4t, bn_axis, start_channel * 16)\n",
    "b5t = up_trans_block(b5d, bn_axis, start_channel * 16)\n",
    "\n",
    "b6d = dense_block(b5t, bn_axis, start_channel * 32, DO)\n",
    "\n",
    "b7t = down_trans_block(b6d, b5d, bn_axis, start_channel * 16)\n",
    "b7d = dense_block(b7t, bn_axis, start_channel * 16)\n",
    "\n",
    "b8t = down_trans_block(b7d, b4d, bn_axis, start_channel * 8)\n",
    "b8d = dense_block(b8t, bn_axis, start_channel * 8)\n",
    "\n",
    "b9t = down_trans_block(b8d, b3d, bn_axis, start_channel * 4)\n",
    "b9d = dense_block(b9t, bn_axis, start_channel * 4)\n",
    "\n",
    "b10t = down_trans_block(b9d, b2d, bn_axis, start_channel * 2)\n",
    "b10d = dense_block(b10t, bn_axis, start_channel * 2)\n",
    "\n",
    "b11t = down_trans_block(b10d, b1d, bn_axis, start_channel)\n",
    "b11d = dense_block(b11t, bn_axis, start_channel)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (b11d)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, bn_axis, channel):\n",
    "    x1 = Conv2D(channel, (3, 3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x1)\n",
    "    x1 = Conv2D(channel, (3, 3), activation='relu', padding='same', use_bias=False)(x1)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x1)\n",
    "    x1 = Conv2D(channel, (3, 3), padding='same', use_bias=False)(x1)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x1)\n",
    "    x = Concatenate(axis=bn_axis)([x, x1])\n",
    "    x = Conv2D(channel, (3, 3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_trans_block(x, dropout=0):\n",
    "    p = MaxPooling2D((2, 2))(x)\n",
    "    d = Dropout(dropout)(p)   if dropout else p\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_trans_block(x, y, channel, upsampling=args.upsample, dropout=0):\n",
    "    if upsampling:\n",
    "        c = UpSampling2D()(x)\n",
    "    else:\n",
    "        c = Conv2DTranspose(channel, (2, 2), strides=(2, 2), padding='same') (x)\n",
    "    c = Conv2D(channel, 2, activation='relu', padding='same')(c)\n",
    "    u = Concatenate(axis=bn_axis)([c, y])\n",
    "    d = Dropout(dropout)(u)   if dropout else u\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft 1 of the net:\n",
    "```python\n",
    "b1d0 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)\n",
    "b1d = Concatenate(axis=bn_axis)([b1d0, inputs])\n",
    "b1t = up_trans_block(b1d, bn_axis, 16)\n",
    "\n",
    "b2d = dense_block(b1t, bn_axis, 16)\n",
    "b2t = up_trans_block(b2d, bn_axis, 32)\n",
    "\n",
    "b3d = dense_block(b2t, bn_axis, 32)\n",
    "b3t = up_trans_block(b3d, bn_axis, 64)\n",
    "\n",
    "b4d = dense_block(b3t, bn_axis, 64)\n",
    "b4t = down_trans_block(b4d, b3d, bn_axis, 32)\n",
    "\n",
    "b5d = dense_block(b4t, bn_axis, 32)\n",
    "b5t = down_trans_block(b5d, b2d, bn_axis, 16)\n",
    "\n",
    "b6d = dense_block(b5t, bn_axis, 16)\n",
    "b6t = down_trans_block(b6d, b1d, bn_axis, 8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft 2 of the net:\n",
    "```python\n",
    "b1d = dense_block(inputs, bn_axis, 8)\n",
    "b1t = up_trans_block(b1d, bn_axis, 8)\n",
    "\n",
    "b2d = dense_block(b1t, bn_axis, 16)\n",
    "b2t = up_trans_block(b2d, bn_axis, 16)\n",
    "\n",
    "b3d = dense_block(b2t, bn_axis, 32)\n",
    "b3t = up_trans_block(b3d, bn_axis, 32)\n",
    "\n",
    "b4d = dense_block(b3t, bn_axis, 64)\n",
    "b4t = up_trans_block(b4d, bn_axis, 64)\n",
    "\n",
    "b5d = dense_block(b4t, bn_axis, 128)\n",
    "\n",
    "b6t = down_trans_block(b5d, b4d, bn_axis, 64)\n",
    "b6d = dense_block(b6t, bn_axis, 64)\n",
    "\n",
    "b7t = down_trans_block(b6d, b3d, bn_axis, 32)\n",
    "b7d = dense_block(b7t, bn_axis, 32)\n",
    "\n",
    "b8t = down_trans_block(b7d, b2d, bn_axis, 16)\n",
    "b8d = dense_block(b8t, bn_axis, 16)\n",
    "\n",
    "b9t = down_trans_block(b8d, b1d, bn_axis, 8)\n",
    "b9d = dense_block(b9t, bn_axis, 8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((IMAGE_SIZE, IMAGE_SIZE, 2))\n",
    "\n",
    "bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "start_channel = 16\n",
    "\n",
    "b1d = dense_block(inputs, bn_axis, start_channel)\n",
    "b1t = up_trans_block(b1d, DO/2)\n",
    "\n",
    "b2d = dense_block(b1t, bn_axis, start_channel * 2)\n",
    "b2t = up_trans_block(b2d, DO)\n",
    "\n",
    "b3d = dense_block(b2t, bn_axis, start_channel * 4)\n",
    "b3t = up_trans_block(b3d, DO)\n",
    "\n",
    "b4d = dense_block(b3t, bn_axis, start_channel * 8)\n",
    "b4t = up_trans_block(b4d, DO)\n",
    "\n",
    "b5d = dense_block(b4t, bn_axis, start_channel * 16)\n",
    "b5t = up_trans_block(b5d, DO)\n",
    "\n",
    "b6d = dense_block(b5t, bn_axis, start_channel * 32)\n",
    "\n",
    "b7t = down_trans_block(b6d, b5d, start_channel * 16, DO)\n",
    "b7d = dense_block(b7t, bn_axis, start_channel * 16)\n",
    "\n",
    "b8t = down_trans_block(b7d, b4d, start_channel * 8, DO)\n",
    "b8d = dense_block(b8t, bn_axis, start_channel * 8)\n",
    "\n",
    "b9t = down_trans_block(b8d, b3d, start_channel * 4, DO)\n",
    "b9d = dense_block(b9t, bn_axis, start_channel * 4)\n",
    "\n",
    "b10t = down_trans_block(b9d, b2d, start_channel * 2, DO)\n",
    "b10d = dense_block(b10t, bn_axis, start_channel * 2)\n",
    "\n",
    "b11t = down_trans_block(b10d, b1d, start_channel, DO)\n",
    "b11d = dense_block(b11t, bn_axis, start_channel)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (b11d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "saltModel = Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible Optimizers:\n",
    "```python\n",
    "LR = 3e-3\n",
    "Adadelta(lr=LR, rho=0.95, epsilon=1e-6, decay=LR/5)  \n",
    "Adam(lr=LR, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay=LR/5)\n",
    "SGD(lr = 0.1, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "SGD(lr = LR, decay=LR/5, momentum=0.9, nesterov=True)\n",
    "RMSprop(lr=LR, rho=0.95, epsilon=1e-6, decay=LR/5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model (I'm using Adam optimizer and mean_iou accuracy for now)\n",
    "saltModel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[mean_iou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a summary of our model just to know what it's doing\n",
    "saltModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we finally fit the model. Notice that we add an early stopper and a check pointer.\n",
    "earlystopper = EarlyStopping(patience=10, verbose=1)\n",
    "checkpointer = ModelCheckpoint('model-tgs-salt-1.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reducelrer = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000005, verbose=1)\n",
    "cb = []\n",
    "if args.reducelr == 0:\n",
    "    cb = [checkpointer]\n",
    "    print(\"no reducelr\")\n",
    "else:\n",
    "    cb = [reducelrer, checkpointer]\n",
    "    print(\"reducing lr\")\n",
    "\n",
    "DPT_SIZE = 4\n",
    "print(X_train_augm.shape)\n",
    "print(DO)\n",
    "results = saltModel.fit(X_train_augm,\n",
    "                    Y_train_augm, \n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=cb)                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fig, (ax_loss, ax_iou) = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "_ = ax_loss.plot(results.epoch, results.history[\"loss\"], label=\"Train loss\")\n",
    "_ = ax_loss.plot(results.epoch, results.history[\"val_loss\"], label=\"Validation loss\")\n",
    "_ = ax_loss.legend()\n",
    "_ = ax_loss.set_title('Loss')\n",
    "#_ = ax_acc.plot(results.epoch, results.history[\"acc\"], label=\"Train accuracy\")\n",
    "#_ = ax_acc.plot(results.epoch, results.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "#_ = ax_acc.legend()\n",
    "#_ = ax_acc.set_title('Accuracy')\n",
    "_ = ax_iou.plot(results.epoch, results.history[\"mean_iou\"], label=\"Train IoU\")\n",
    "_ = ax_iou.plot(results.epoch, results.history[\"val_mean_iou\"], label=\"Validation IoU\")\n",
    "_ = ax_iou.legend()\n",
    "_ = ax_iou.set_title('IoU')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check performance on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltModel = load_model('model-tgs-salt-1.h5', custom_objects={'mean_iou': mean_iou})\n",
    "saltModel.evaluate(x_valid, y_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = saltModel.predict(x_valid, \n",
    "                             verbose=1).reshape(-1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the model so you can use the best IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(labels, y_pred, print_table=False):\n",
    "    \"\"\"\n",
    "    src: https://www.kaggle.com/aglotero/another-iou-metric\"\"\"\n",
    "    class_bins = 2\n",
    "\n",
    "    # H : ndarray, shape(nx, ny)\n",
    "    # The bi-dimensional histogram of samples x and y. \n",
    "    # Values in x are histogrammed along the first dimension and values in y are histogrammed along the second dimension.\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(class_bins, class_bins))[0] # was 0\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins=class_bins)[0] # was 0 (0: no mask, 1: mask)\n",
    "    area_pred = np.histogram(y_pred, bins=class_bins)[0] # was 0\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    \"\"\"Compute IoU batchwise\"\"\"\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    return np.mean([iou_metric(y_true_in[b], y_pred_in[b]) for b in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.1, 0.9, 40)\n",
    "ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) \n",
    "                 for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious)\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_ = plt.plot(thresholds, ious)\n",
    "_ = plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "_ = plt.xlabel(\"Threshold\")\n",
    "_ = plt.ylabel(\"IoU\")\n",
    "_ = plt.title(\"Threshold: {} delivers best mean-IoU: {} \".format(threshold_best.round(2), iou_best.round(2)))\n",
    "_ = plt.legend()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now it is time to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We read in the test set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [upsample(np.array(load_img(os.path.join(test_dir, '%s.png' % idx), grayscale=True))) / 255 \n",
    "                   for idx in tqdm_notebook(test_df.index)]\n",
    "x_test = np.array(x_test).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "print(x_test.shape)\n",
    "#np.ones_like(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[((np.ones_like(x_test[i]) * test_df.loc[i][\"z\"]) - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH)\n",
    "                     for i in tqdm_notebook(test_df.index)] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create depth layer\n",
    "x_test_d = [np.ones((128,128,1)) * ((test_df.loc[i][\"z\"] - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH))\n",
    "                     for i in tqdm_notebook(test_df.index)]\n",
    "print(x_test_d[0].shape)\n",
    "x_test_d = np.array(x_test_d).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "print(x_test_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_df[\"images\"] = [np.array(load_img(os.path.join(train_dir_img, '%s.png' % idx), grayscale=True)) / 255 \n",
    "                      for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(os.path.join(train_dir_mask, '%s.png' % idx), grayscale=True)) / 255 \n",
    "                     for idx in tqdm_notebook(train_df.index)]\n",
    "rain_df[\"depth\"] = [((np.ones_like(train_df.loc[i][\"images\"]) * train_df.loc[i][\"z\"]) - MIN_DEPTH)/(MAX_DEPTH - MIN_DEPTH)\n",
    "                     for i in tqdm_notebook(train_df.index)]\n",
    "print(train_df[\"depth\"][0].shape)\n",
    "train_df[\"images_d\"] = [np.dstack((train_df[\"images\"][i], train_df[\"depth\"][i])) for i in tqdm_notebook(train_df.index)]\n",
    "\n",
    "x_test_d = [np.ones((128,128,1)) * ((test_df.loc[i][\"z\"] - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH))\n",
    "                     for i in tqdm_notebook(test_df.index)]\n",
    "                     \n",
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, depth_train, depth_valid = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images_d.map(upsample).tolist()).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 2), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1), \n",
    "    train_df.z.values,\n",
    "    test_size=0.2, \n",
    "    stratify=train_df.masks.map(coverage_class), \n",
    "    random_state=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on train, val and test\n",
    "saltModel = load_model('model-tgs-salt-1.h5', custom_objects={'mean_iou': mean_iou})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "print(x_test[0].shape)\n",
    "print(x_test_d.shape)\n",
    "print(x_test_d[0].shape)\n",
    "x_test_full = [np.dstack((elem, x_test_d[idx])) for idx, elem in enumerate(x_test)]\n",
    "print(x_test_full[0].shape) \n",
    "x_test_full = np.array(x_test_full)\n",
    "print(x_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = saltModel.predict(x_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = next(os.walk(test_dir))[2]\n",
    "assert len(set(test_ids) ^ set(test_df.index+'.png')) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original rlenc\n",
    "```python\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLenc(img, order='F'):\n",
    "    \"\"\"Convert binary mask image to run-length array or string.\n",
    "    \n",
    "    Args:\n",
    "    img: image in shape [n, m]\n",
    "    order: is down-then-right, i.e. Fortran(F)\n",
    "    string: return in string or array\n",
    "\n",
    "    Return:\n",
    "    run-length as a string: <start[1s] length[1s] ... ...>\n",
    "    \"\"\"\n",
    "    bytez = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    bytez = np.concatenate([[0], bytez, [0]])\n",
    "    runs = np.where(bytez[1:] != bytez[:-1])[0] + 1 # pos start at 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# Use for sanity check the encode function\n",
    "def RLdec(rl_string, shape=(101, 101), order='F'):\n",
    "    \"\"\"Convert run-length string to binary mask image.\n",
    "    \n",
    "    Args:\n",
    "    rl_string: \n",
    "    shape: target shape of array\n",
    "    order: decode order is down-then-right, i.e. Fortran(F)\n",
    "\n",
    "    Return:\n",
    "    binary mask image as array\n",
    "    \"\"\"\n",
    "    s = rl_string.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) \n",
    "             for i, idx in enumerate(tqdm_notebook(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly, we prepare the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission naming process before splitting:\n",
    "```python\n",
    "'saltSubmission_8.14_%s_batch%s_epoch%s.csv' % (optimizer, BATCH_SIZE, EPOCHS)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "\n",
    "sub.to_csv(os.path.join(model_dir, 'saltSubmission_' + prefix + info))\n",
    "\n",
    "print('Prediction result saved as saltSubmission_' + prefix + info)\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
