{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model of Salt Identification with Keras ConvNet\n",
    "Transfer learning will probably be harder with the wonky pictures of salt identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# % matplotlib inline\n",
    "import datetime as dt\n",
    "\n",
    "# Import plotting for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 16\n",
    "#from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pydot\n",
    "import random, math\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from keras.applications import xception\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Concatenate\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "import sys\n",
    "sys.path.append(\"/utils/DLWorkspace-Utils/keras-multiprocess-image-data-generator/tools\")\n",
    "import image as T\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix # for the confusion matrix\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import warnings\n",
    "import cv2\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's set up our optimizer so we can choose which one later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=4, decay=0.0001, drops_epochs=0, gpus=1, lr=0.1, nb_epochs=2, optimizer='adadelta', split=None)\n"
     ]
    }
   ],
   "source": [
    "optimizer_collections = {\n",
    "    \"adadelta\" : Adadelta(), \n",
    "    \"nadam\" : Nadam(), \n",
    "    \"rmsprop\": RMSprop(), \n",
    "    \"adam\": Adam(), \n",
    "    \"adagrad\": Adagrad(), \n",
    "    \"adamax\": Adamax(), \n",
    "}\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=8, help='Batch size', type=int)\n",
    "parser.add_argument('--nb_epochs', default=120, help='Number of Epochs', type=int)\n",
    "parser.add_argument('--optimizer', default='adadelta', help='Optimizer', type=str)\n",
    "parser.add_argument('--split', default=None, help='Which data set to use', type=int)\n",
    "parser.add_argument('--decay', default=1e-4, help='Rate decay', type=float)\n",
    "parser.add_argument('--gpus', default=1, help='Number of GPU', type=int)\n",
    "parser.add_argument('--drops_epochs', default=0, help='Epochs which rate drop by x10', type=float)\n",
    "parser.add_argument('--lr', default=0.1, help='Learning Rate', type=float)\n",
    "\n",
    "#args = parser.parse_args(\"--optimizer adadelta\".split())\n",
    "args = parser.parse_args()\n",
    "\n",
    "print( args )\n",
    "batch_size = 8\n",
    "if args.batch_size:\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "num_gpu = args.gpus\n",
    "# Number of training epochs\n",
    "fit_epochs = args.nb_epochs\n",
    "# data to use. \n",
    "split = args.split\n",
    "\n",
    "lr_top = args.lr\n",
    "\n",
    "if args.optimizer.startswith(\"sgd\"):\n",
    "    optimizer = args.optimizer\n",
    "    opt = SGD(lr = lr_top, decay=args.decay, momentum=0.9, nesterov=True)\n",
    "else:\n",
    "    optimizer = args.optimizer\n",
    "    opt = optimizer_collections[args.optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a tee object to help write log output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tee(object):\n",
    "    def __init__(self, name):\n",
    "        self.file = open(name, \"w\")\n",
    "        self.stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "    def __del__(self):\n",
    "        sys.stdout = self.stdout\n",
    "        self.file.close()\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "    def flush(self):\n",
    "        self.file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the dataset\n",
    "First, we locate our data at the correct directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/kaggle/competitions/tgs-salt-identification-challenge/train\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/kaggle/competitions/tgs-salt-identification-challenge/'\n",
    "cur_dir = '/work/kaggle-practice/tgs_salt_identification/'\n",
    "result_dir = os.path.join(cur_dir, 'result')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "train_dir_img = os.path.join(train_dir, 'images')\n",
    "train_dir_mask = os.path.join(train_dir, 'masks')\n",
    "print(train_dir)\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "test_dir = os.path.join(test_dir, 'images')\n",
    "#sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a little piece of code if we wanted to test out different lrs and decays\n",
    "```python\n",
    "if os.path.isdir(os.path.join(result_dir, \"tgs-salt_%s_batch%s_epoch%s_lr%s_decay%s\" % (optimizer, batch_size, fit_epochs, lr_top, args.decay))) == False:\n",
    "    os.mkdir(os.path.join(result_dir, \"tgs-salt_%s_batch%s_epoch%s_lr%s_decay%s\" % (optimizer, batch_size, fit_epochs, lr_top, args.decay)))\n",
    "    \n",
    "model_dir = os.path.join(result_dir, \"tgs-salt_%s_batch%s_epoch%s_lr%s_decay%s\" % (optimizer, batch_size, fit_epochs, lr_top, args.decay))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 959, Min: 50\n",
      "4000\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 112, 112, 2)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 112, 112, 8)  152         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 112, 112, 10) 0           conv2d_15[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 112, 112, 10) 40          concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 112, 112, 10) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 112, 112, 16) 1456        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 56, 56, 16)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 56, 56, 16)   64          average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 56, 56, 16)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 56, 56, 16)   2304        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 56, 56, 32)   0           average_pooling2d_7[0][0]        \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 56, 56, 32)   128         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 56, 56, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 56, 56, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 28, 28, 32)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 28, 28, 32)   128         average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 32)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 28, 28, 32)   9216        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 28, 28, 64)   0           average_pooling2d_8[0][0]        \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 28, 28, 64)   256         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 28, 28, 64)   36928       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 14, 14, 64)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 14, 14, 64)   256         average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 14, 14, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 14, 14, 64)   36864       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 14, 14, 128)  0           average_pooling2d_9[0][0]        \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 14, 14, 128)  512         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 14, 14, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 32)   16416       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 28, 28, 96)   0           conv2d_transpose_3[0][0]         \n",
      "                                                                 concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 28, 28, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 28, 28, 96)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 32)   27648       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 28, 28, 128)  0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 28, 28, 128)  512         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 28, 28, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 56, 56, 16)   8208        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 56, 56, 48)   0           conv2d_transpose_4[0][0]         \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 56, 56, 48)   192         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 56, 56, 48)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 56, 56, 16)   6912        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 56, 56, 64)   0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 56, 56, 64)   256         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 56, 56, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 112, 112, 8)  2056        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 112, 112, 18) 0           conv2d_transpose_5[0][0]         \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 112, 112, 1)  19          concatenate_19[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 160,155\n",
      "Trainable params: 158,791\n",
      "Non-trainable params: 1,364\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3600 samples, validate on 400 samples\n",
      "Epoch 1/2\n",
      "3600/3600 [==============================] - 34s 10ms/step - loss: 0.3526 - mean_iou: 0.4252 - val_loss: 0.2920 - val_mean_iou: 0.4812\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29197, saving model to model-tgs-salt-1.h5\n",
      "Epoch 2/2\n",
      "3600/3600 [==============================] - 30s 8ms/step - loss: 0.2729 - mean_iou: 0.5166 - val_loss: 0.2639 - val_mean_iou: 0.5465\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.29197 to 0.26393, saving model to model-tgs-salt-1.h5\n",
      "3600/3600 [==============================] - 4s 1ms/step\n",
      "400/400 [==============================] - 0s 962us/step\n",
      "18000/18000 [==============================] - 17s 950us/step\n",
      "\n",
      "\n",
      "Prediction result saved as saltSubmission.csv\n"
     ]
    }
   ],
   "source": [
    "# Set up location of output\n",
    "if os.path.isdir(os.path.join(result_dir, \"dtgs-salt_%s_batch%s_epoch%s\" % (optimizer, batch_size, fit_epochs))) == False:\n",
    "    os.mkdir(os.path.join(result_dir, \"dtgs-salt_%s_batch%s_epoch%s\" % (optimizer, batch_size, fit_epochs)))\n",
    "model_dir = os.path.join(result_dir, \"dtgs-salt_%s_batch%s_epoch%s\" % (optimizer, batch_size, fit_epochs))\n",
    "if os.path.exists(os.path.join(model_dir, \"training_log\")) == True:\n",
    "    os.remove(os.path.join(model_dir, \"training_log\"))\n",
    "log_file = os.path.join(model_dir, \"training_log\")\n",
    "sys.stdout = Tee(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nids= ['1f1cc6b3a4','5b7c160d0d','6c40978ddf','7dfdf6eeb8','7e5a6e5013']\\nplt.figure(figsize=(20,10))\\nfor j, img_name in enumerate(ids):\\n    q = j+1\\n    img = T.load_img(os.path.join(train_dir_img, img_name + '.png'))\\n    img_mask = T.load_img(os.path.join(train_dir_mask, img_name + '.png'))\\n    \\n    plt.subplot(1,2*(1+len(ids)),q*2-1)\\n    plt.imshow(img)\\n    plt.subplot(1,2*(1+len(ids)),q*2)\\n    plt.imshow(img_mask)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ids= ['1f1cc6b3a4','5b7c160d0d','6c40978ddf','7dfdf6eeb8','7e5a6e5013']\n",
    "plt.figure(figsize=(20,10))\n",
    "for j, img_name in enumerate(ids):\n",
    "    q = j+1\n",
    "    img = T.load_img(os.path.join(train_dir_img, img_name + '.png'))\n",
    "    img_mask = T.load_img(os.path.join(train_dir_mask, img_name + '.png'))\n",
    "    \n",
    "    plt.subplot(1,2*(1+len(ids)),q*2-1)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1,2*(1+len(ids)),q*2)\n",
    "    plt.imshow(img_mask)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some examples of depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4ac19fb269</th>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825fadf99</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f59821d067</th>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5b435fad9d</th>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e340e7bfca</th>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              z\n",
       "id             \n",
       "4ac19fb269  306\n",
       "1825fadf99  157\n",
       "f59821d067  305\n",
       "5b435fad9d  503\n",
       "e340e7bfca  783"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_depths = pd.read_csv(os.path.join(data_dir, \"depths.csv\"), index_col='id')\n",
    "depths_max = df_depths['z'].max()\n",
    "depths_min = df_depths['z'].min()\n",
    "print(\"Max: %s, Min: %s\" % (depths_max, depths_min))\n",
    "df_depths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also need to make some ImageDataGenerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_datagen = T.ImageDataGenerator(\\n        rotation_range=90,\\n        width_shift_range=0.2,\\n        height_shift_range=0.2,\\n        rescale=1./255,\\n        shear_range=0.2,\\n        zoom_range=0.2,\\n        horizontal_flip=True,\\n        vertical_flip=True,\\n        fill_mode=\"constant\",\\n        cval=0)\\ntrain_datagen_unch = T.ImageDataGenerator(rescale=1./255)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_datagen = T.ImageDataGenerator(\n",
    "        rotation_range=90,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode=\"constant\",\n",
    "        cval=0)\n",
    "train_datagen_unch = T.ImageDataGenerator(rescale=1./255)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with the training set.\n",
    "Here's code, for the ImageDataGenerators:\n",
    "```python\n",
    "train_generator_img_unch = train_datagen_unch.flow_from_directory(\n",
    "        train_dir_img,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE, 1),\n",
    "        batch_size= batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode=None)\n",
    "train_generator_mask_unch = train_datagen_unch.flow_from_directory(\n",
    "        train_dir_mask,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE, 1),\n",
    "        batch_size= batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode=None)\n",
    "\n",
    "train_generator_img = train_datagen.flow_from_directory(\n",
    "        train_dir_img,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE, 1),\n",
    "        batch_size= batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode=None)\n",
    "train_generator_mask = train_datagen.flow_from_directory(\n",
    "        train_dir_mask,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE, 1),\n",
    "        batch_size= batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode=None)\n",
    "```\n",
    "\n",
    "And before I padded the pictures and added depth:\n",
    "```python\n",
    "for file in os.listdir(train_dir_img):\n",
    "    x = load_img(os.path.join(train_dir_img, file))\n",
    "    x = img_to_array(x)[:,:,1]\n",
    "    x = resize(x, (IMAGE_SIZE, IMAGE_SIZE, 1), mode='constant', preserve_range=True)\n",
    "    train_x[count] = x\n",
    "    mask = load_img(os.path.join(train_dir_mask, file))\n",
    "    mask = img_to_array(mask)[:,:,1]\n",
    "    train_y[count] = resize(mask, (IMAGE_SIZE, IMAGE_SIZE, 1), mode='constant', preserve_range=True)\n",
    "    count += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PICS = len(os.listdir(train_dir_img))\n",
    "print(TRAIN_PICS)\n",
    "IMAGE_SIZE = 112 # pixel height and width of each image--actual size of the images is 101\n",
    "\n",
    "# Get and resize train images and masks\n",
    "train_x = np.zeros((TRAIN_PICS, IMAGE_SIZE, IMAGE_SIZE, 2), dtype=np.float32)\n",
    "train_y = np.zeros((TRAIN_PICS, IMAGE_SIZE, IMAGE_SIZE, 1), dtype=np.bool)\n",
    "count = 0\n",
    "for file in os.listdir(train_dir_img):\n",
    "    # First, the images. \n",
    "    # Read them...\n",
    "    x = load_img(os.path.join(train_dir_img, file))\n",
    "    x = img_to_array(x)[:,:,1]\n",
    "    x = x / 255\n",
    "    cur_size = x.shape[0]\n",
    "    # ...pad them so that it's divisible by 16 and will fit the U-Net...\n",
    "    img = np.zeros((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    start = math.floor((IMAGE_SIZE-cur_size)/2)\n",
    "    end = IMAGE_SIZE - math.ceil((IMAGE_SIZE-cur_size)/2)\n",
    "    img[start:end, start:end] = x\n",
    "    # ...and add depth.\n",
    "    img_depth = df_depths.loc[file.replace('.png', ''), 'z']\n",
    "    rescaled_depth = (img_depth - depths_min)/(depths_max - depths_min)\n",
    "    depth = np.full((IMAGE_SIZE, IMAGE_SIZE), rescaled_depth)\n",
    "    train_x[count] = np.stack((img, depth), axis=2)\n",
    "    \n",
    "    # Now, the masks.\n",
    "    # Read them...\n",
    "    y = load_img(os.path.join(train_dir_mask, file))\n",
    "    y = img_to_array(y)[:,:,1]\n",
    "    # ...pad them so that they're divisible by 16 and will fit the U-Net.\n",
    "    mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    mask[start:end, start:end] = y\n",
    "    train_y[count] = np.expand_dims(mask, axis=2)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we read it in correctly by visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nix = random.randint(0, TRAIN_PICS)\\nprint(\"Index: \" + str(ix))\\nplt.imshow(np.dstack((train_x[ix], train_x[ix], train_x[ix])))\\nplt.show()\\ntmp = np.squeeze(train_y[ix]).astype(np.float32)\\nplt.imshow(np.dstack((tmp, tmp, tmp)))\\nplt.show()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ix = random.randint(0, TRAIN_PICS)\n",
    "print(\"Index: \" + str(ix))\n",
    "plt.imshow(np.dstack((train_x[ix], train_x[ix], train_x[ix])))\n",
    "plt.show()\n",
    "tmp = np.squeeze(train_y[ix]).astype(np.float32)\n",
    "plt.imshow(np.dstack((tmp, tmp, tmp)))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Some training data examples.\")\\nimg_batch, img_unch_batch = next(train_generator_img), next(train_generator_img_unch)\\nmask_batch, mask_unch_batch = next(train_generator_mask), next(train_generator_mask_unch)\\nfor i in range (0,1):\\n    image = img_batch[i]\\n    print(\"The new image:\")\\n    plt.imshow(image) # .transpose(2,1,0)\\n    \\n    image_unch = img_unch_batch[i]\\n    print(\"The unchanged image:\")\\n    plt.imshow(image_unch) # .transpose(2,1,0)\\n    \\n    mask = mask_batch[i]\\n    print(\"The new mask:\")\\n    plt.imshow(mask) # .transpose(2,1,0)\\n    \\n    mask_unch = mask_unch_batch[i]\\n    print(\"The unchanged mask:\")\\n    plt.imshow(mask_unch) # .transpose(2,1,0)\\n    \\n    plt.show()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"Some training data examples.\")\n",
    "img_batch, img_unch_batch = next(train_generator_img), next(train_generator_img_unch)\n",
    "mask_batch, mask_unch_batch = next(train_generator_mask), next(train_generator_mask_unch)\n",
    "for i in range (0,1):\n",
    "    image = img_batch[i]\n",
    "    print(\"The new image:\")\n",
    "    plt.imshow(image) # .transpose(2,1,0)\n",
    "    \n",
    "    image_unch = img_unch_batch[i]\n",
    "    print(\"The unchanged image:\")\n",
    "    plt.imshow(image_unch) # .transpose(2,1,0)\n",
    "    \n",
    "    mask = mask_batch[i]\n",
    "    print(\"The new mask:\")\n",
    "    plt.imshow(mask) # .transpose(2,1,0)\n",
    "    \n",
    "    mask_unch = mask_unch_batch[i]\n",
    "    print(\"The unchanged mask:\")\n",
    "    plt.imshow(mask_unch) # .transpose(2,1,0)\n",
    "    \n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for the actual model!\n",
    "For now, I'm basing my model off of Jesper's U-Net model, which is in turn based off of Ketil's model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, an Intersection over Union metric to calculate the accuracy of our identification.\n",
    "Remember, IoU helps in object detection by figuring out the similarity (intersection/union) of two bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now the model!\n",
    "My original U-Net model before melding with DenseNet:\n",
    "```python\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (s)\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, bn_axis, channel):\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = Conv2D(channel, 3, padding='same', use_bias=False)(x1)\n",
    "    x = Concatenate(axis=bn_axis)([x, x1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_trans_block(x, bn_axis, channel):\n",
    "    t = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    t = Activation('relu')(t)\n",
    "    c = Conv2D(channel, (3, 3), activation='relu', padding='same') (t)\n",
    "    p = AveragePooling2D(2, strides=2)(c)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_trans_block(x, y, bn_axis, channel):\n",
    "    t = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n",
    "    t = Activation('relu')(t)\n",
    "    c = Conv2DTranspose(channel, (2, 2), strides=(2, 2), activation='relu', padding='same') (t)\n",
    "    u6 = Concatenate(axis=bn_axis)([c, y])\n",
    "    return u6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((IMAGE_SIZE, IMAGE_SIZE, 2))\n",
    "\n",
    "bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "b1d0 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)\n",
    "b1d = Concatenate(axis=bn_axis)([b1d0, inputs])\n",
    "b1t = up_trans_block(b1d, bn_axis, 16)\n",
    "\n",
    "b2d = dense_block(b1t, bn_axis, 16)\n",
    "b2t = up_trans_block(b2d, bn_axis, 32)\n",
    "\n",
    "b3d = dense_block(b2t, bn_axis, 32)\n",
    "b3t = up_trans_block(b3d, bn_axis, 64)\n",
    "\n",
    "b4d = dense_block(b3t, bn_axis, 64)\n",
    "b4t = down_trans_block(b4d, b3d, bn_axis, 32)\n",
    "\n",
    "b5d = dense_block(b4t, bn_axis, 32)\n",
    "b5t = down_trans_block(b5d, b2d, bn_axis, 16)\n",
    "\n",
    "b6d = dense_block(b5t, bn_axis, 16)\n",
    "b6t = down_trans_block(b6d, b1d, bn_axis, 8)\n",
    "\n",
    "#b7d = denser_block()\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (b6t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "saltModel = Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model (I'm using Adam optimizer and mean_iou accuracy for now)\n",
    "saltModel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[mean_iou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a summary of our model just to know what it's doing\n",
    "saltModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we finally fit the model. Notice that we add an early stopper and a check pointer.\n",
    "#earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint('model-tgs-salt-1.h5', verbose=1, save_best_only=True)\n",
    "results = saltModel.fit(train_x, train_y, validation_split=0.1, batch_size=batch_size, epochs=fit_epochs, \n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now it is time to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We read in the test set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PICS = len(os.listdir(test_dir))\n",
    "test = np.zeros((TEST_PICS, IMAGE_SIZE, IMAGE_SIZE, 2), dtype=np.uint8)\n",
    "test_ids = []\n",
    "sizes_test = []\n",
    "count = 0\n",
    "for file in os.listdir(test_dir):\n",
    "    # Read the images...\n",
    "    x = load_img(os.path.join(test_dir, file))\n",
    "    x = img_to_array(x)[:,:,1]\n",
    "    x = x / 255\n",
    "    cur_size = x.shape[0]\n",
    "    sizes_test.append([x.shape[0], x.shape[1]])\n",
    "    # ...pad them so that it's divisible by 16 and will fit the U-Net...\n",
    "    img = np.zeros((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    start = math.floor((IMAGE_SIZE-cur_size)/2)\n",
    "    end = IMAGE_SIZE - math.ceil((IMAGE_SIZE-cur_size)/2)\n",
    "    img[start:end, start:end] = x\n",
    "    # ...and add depth.\n",
    "    img_depth = df_depths.loc[file.replace('.png', ''), 'z']\n",
    "    rescaled_depth = (img_depth - depths_min)/(depths_max - depths_min)\n",
    "    depth = np.full((IMAGE_SIZE, IMAGE_SIZE), rescaled_depth)\n",
    "    test[count] = np.stack((img, depth), axis=2)\n",
    "    test_ids.append(file) # Images' ids\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's print our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nix = random.randint(0, TEST_PICS)\\nprint(\"Index: \" + str(ix))\\nplt.imshow(np.dstack((test[ix], test[ix], test[ix])))\\nplt.show()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ix = random.randint(0, TEST_PICS)\n",
    "print(\"Index: \" + str(ix))\n",
    "plt.imshow(np.dstack((test[ix], test[ix], test[ix])))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on train, val and test\n",
    "saltModel = load_model('model-tgs-salt-1.h5', custom_objects={'mean_iou': mean_iou})\n",
    "preds_train = saltModel.predict(train_x[:int(train_x.shape[0]*0.9)], verbose=1)\n",
    "preds_val = saltModel.predict(train_x[int(train_x.shape[0]*0.9):], verbose=1)\n",
    "preds_test = saltModel.predict(test, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517875e78dfd4693862ac3117ace85eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/usr/local/lib/python3.5/dist-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create list of upsampled test masks\n",
    "preds_test_upsampled = []\n",
    "for i in tnrange(len(preds_test)):\n",
    "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n",
    "                                       (sizes_test[i][0], sizes_test[i][1]), \n",
    "                                       mode='constant', preserve_range=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Perform a sanity check on some random training samples\\nix = random.randint(0, len(preds_train_t))\\nprint(\"Index: \" + str(ix))\\nprint(\"\\nOriginal Image:\")\\nplt.imshow(np.dstack((train_x[ix], train_x[ix], train_x[ix])))\\nplt.show()\\nprint(\"\\nExpected Mask:\")\\ntmp = np.squeeze(train_y[ix]).astype(np.float32)\\nplt.imshow(np.dstack((tmp, tmp, tmp)))\\nplt.show()\\nprint(\"\\nActual Mask:\")\\ntmp = np.squeeze(preds_train_t[ix]).astype(np.float32)\\nplt.imshow(np.dstack((tmp, tmp, tmp)))\\nplt.show()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Perform a sanity check on some random training samples\n",
    "ix = random.randint(0, len(preds_train_t))\n",
    "print(\"Index: \" + str(ix))\n",
    "print(\"\\nOriginal Image:\")\n",
    "plt.imshow(np.dstack((train_x[ix], train_x[ix], train_x[ix])))\n",
    "plt.show()\n",
    "print(\"\\nExpected Mask:\")\n",
    "tmp = np.squeeze(train_y[ix]).astype(np.float32)\n",
    "plt.imshow(np.dstack((tmp, tmp, tmp)))\n",
    "plt.show()\n",
    "print(\"\\nActual Mask:\")\n",
    "tmp = np.squeeze(preds_train_t[ix]).astype(np.float32)\n",
    "plt.imshow(np.dstack((tmp, tmp, tmp)))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly, we prepare the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c70deb755f74f5fbeabcdc2ec5e13e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs\n",
    "\n",
    "pred_dict = {fn[:-4]:RLenc(np.round(preds_test_upsampled[i])) for i,fn in tqdm_notebook(enumerate(test_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "\n",
    "sub.to_csv(os.path.join(model_dir, 'saltSubmission.csv'))\n",
    "\n",
    "print('Prediction result saved as saltSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
